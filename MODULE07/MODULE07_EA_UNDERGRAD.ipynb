{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"gray\">DOING COMPUTATIONAL SOCIAL SCIENCE<br>MODULE 7 <strong>PROBLEM SETS</strong></font>\n",
    "\n",
    "# <font color=\"#49699E\" size=40>Text Analysis, Part 2</font>\n",
    "This module notebook assignment is organized into two parts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What You Need to Know Before Getting Started\n",
    "\n",
    "- **You can consult any resources you want when completing these exercises and problems**. Just as it is in the \"real world:\" if you can't figure out how to do something, look it up. My recommendation is that you check the relevant parts of the assigned reading or search for inspiration on [https://stackoverflow.com](https://stackoverflow.com).\n",
    "- **Each problem is worth 1 point**. All problems are equally weighted.\n",
    "- **The information you need for each problem set is provided in the blue and green cells.** General instructions / the problem set preamble are in the blue cells, and instructions for specific problems are in the green cells. **You have to execute all of the code in the problem set, but you are only responsible for entering code into the code cells that immediately follow a green cell**. You will also recognize those cells because they will be incomplete. You need to replace each `____` with the code that will make the cell execute properly.\n",
    "- **The comments in the problem cells contain clues indicating what the following line of code is supposed to do.** Use these comments as a guide when filling in the blanks. \n",
    "- **Each problem cell stores one object named according to the problem (e.g. _09)**. These are not important for you, but we use them to help grade your work efficiently, so **do not delete them or change their names**. If you do, you will lose marks.\n",
    "- **You can ask for help**. If you run into problems, you can reach out to John (john.mclevey@uwaterloo.ca) or Pierson (pbrowne@uwaterloo.ca) for help. You can ask a friend for help if you like, regardless of whether they are enrolled in the course.\n",
    "\n",
    "Finally, remember that you do not need to \"master\" this content before moving on to other course materials, as what is introduced here is reinforced throughout the rest of the course. You will have plenty of time to practice and cement your new knowledge and skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Submit Your (Pickled) Assignment! \n",
    "\n",
    "Since we've had to rethink the way we deliver, collect, and evaluate these problem sets, we want to be very clear about what you need to do to properly submit this module notebook assignment. Please read the following explanation of our process so that you understand how this works, and what you need to do.\n",
    "\n",
    "At the very end of this notebook, there is a code cell that will compile all of your answers to every problem in the assignment and save them as a 'pickle' file (`.pkl`) in the current working directory. You can execute that cell as many times as you like. Each time you run it, it will overwrite the old pickle with your updated answers. **Once you've ensured that everything in the notebook is complete and finished to your satisfaction, it's up to you to get the pickle that you just created and upload it to the appropriate Learn dropbox for this module.** The file you are looking for will not exist until you run the cells at the end of the notebook. Once it has been created, it will follow this naming convention: \n",
    "\n",
    "> `module_[number]__student_[your_student_number].pkl`\n",
    "\n",
    "To be very clear, **you need to submit the pickle to Learn**. You do not need to upload the Jupyter Notebook as initially planned. **Just the pickle!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Sure Everything is Good to Go\n",
    "\n",
    "It's generally a good idea to do a 'fresh' run of your entire notebook before you submit your assignment to make sure that everything is working as it should be. You can use the button with the 'Fast-Forward' arrows in the Jupyter toolbar above to restart the kernel (resetting everything to initial conditions) and running every code cell in the notebook, in order. You can also select 'Restart and Run All' from the Kernel dropdown menu. If the entire notebook runs without throwing any errors, you should be good to go!\n",
    "\n",
    "If you're running into issues, make sure that you haven't changed any of the 'answer' variable names we provided you with (e.g., we asked you to store your answer to the first question in a variable called `_01`). If you change an answer's variable name or don't store your answer in that variable, the project won't finalize properly and you won't get proper credit for your work. The same goes for the `student_id` metadata variable we ask you to complete immediately below; if any of those are missing, haven't been filled in properly, or have been renamed, issues may arise during the grading process and you will not receive proper credit. So make sure you enter your student information, and don't delete or change the names of the variables that store your answers to each problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: ADD YOUR STUDENT ID NUMBER\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "To evaluate your work, we need you to provide your student number. In the cell below, <strong>replace '12345678' with your student number</strong>. The student_id' variable needs to be an integer, so <strong>do not wrap it in quotes.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your UWaterloo student ID number\n",
    "student_id = 12345678 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "For these exercises, you are provided a dataframe containing a sample of speeches from the Canadian Hansard data. The speech text is already pre-processed, with bigrams detected and all but nouns, proper nouns, and adjectives filtered out. Using the text from the 'preprocessed' column of the dataframe, you will create a matrix of count vectors, where each row is a speech and the columns are word counts for each item in the vocabulary. You'll then create a dataframe from this matrix, adding a column with the party of the person who made the speech.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Begin by reading the CSV into pandas and extracting the pre-processed speeches as a list. Provide this list of speeches to sklearn's CountVectorizer, turning the results into a new dataframe and naming the columns for the terms they contain counts of. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_01\n",
    "\n",
    "df = pd.read_csv('_____')   # read pre-processed dataset\n",
    "speeches = df['_____'].tolist() # make a list of speeches (you might have to look inside the dataframe)\n",
    "\n",
    "count_vectorizer = _____(max_df=.1,          # initialize the counter vectorizer\n",
    "                           min_df=3,\n",
    "                           strip_accents='ascii',\n",
    "                           )\n",
    "\n",
    "count_matrix = count_vectorizer.fit_transform(_____)   # apply the vectorizer to the list of speeches\n",
    "vocabulary = count_vectorizer._____()        # gather a list of the feature names (terms)\n",
    "\n",
    "count_df = pd.DataFrame.sparse.from_spmatrix(count_matrix)    # use pandas sparse matrix functionality to make a dataframe\n",
    "count_df.columns = vocabulary                            # name the columns according to what they're counting\n",
    "\n",
    "count_df['speakerparty'] = df['speakerparty']     # add the party names from the original dataframe to the new one\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_01 = count_df.head().to_numpy() # do not change this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Combining the count vectors for each of the 3 largest political parties in Canada (Liberal, Conservative, NDP) you will produce a 3 row dataframe where the rows are the composition of each party's speeches in the data. Convert the raw counts to proportions of the party's total words, to more easily compare the party's term vectors to each other.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Use pandas' groupby to create party groups in the dataframe, with their word counts added together as the aggregation function. Transform the dataframe of counts into a dataframe of proportions (ie. term count / total words). Add the terms with the largest percentages for each party to a dictionary and print the items in the dictionary. Submit the dictionary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_02\n",
    "\n",
    "# group the speech vectors by party, adding the counts together\n",
    "party_counts = count_df._____('speakerparty')._____('sum')      \n",
    "\n",
    "# transform the count dataframe to proportions by dividing the values by the total words\n",
    "party_percents = party_counts.div(party_counts.sum(axis=1), axis=0)   \n",
    "party_percents = party_percents._____ # transform the dataframe so that each party is a column and each term a row\n",
    "\n",
    "top_words_per_party = {}\n",
    "\n",
    "# loop through each party in the data, adding their top (term,score) tuples to their dictionary entry\n",
    "for party in party_percents.columns:             \n",
    "    top = party_percents[party].nlargest(10)\n",
    "    top_words_per_party[party] = list(zip(top.index, top))\n",
    "\n",
    "# print the keys (party name) and associated values (top terms) in the dictionary\n",
    "for k, v in top_words_per_party.items():\n",
    "    print(k.upper())\n",
    "    for each in v:\n",
    "        print(each)\n",
    "    print('\\n')\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_02 = top_words_per_party # do not change this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "For this problem, you will find the words that most differentiate each party from each of the other two parties, in terms of proportion of total words.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "By subtracting the party term proportion vectors from each other, gather the terms that are most associated with each side of the comparison. At this point, these vectors should be the columns of the dataframe. Because higher positive values are more associated with one party in the comparison and negative values with the other party, this only requires three comparisons to look at both ends of the party combinations. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_03\n",
    "\n",
    "# term vector calculations and sorting\n",
    "lib_to_con = party_percents['Liberal'] - party_percents['Conservative']  \n",
    "lib_to_con._____(ascending=False, inplace=True)\n",
    "ndp_to_con = party_percents['NDP'] - party_percents['Conservative']\n",
    "ndp_to_con._____(ascending=False, inplace=True)\n",
    "lib_to_ndp = party_percents['Liberal'] - party_percents['NDP']\n",
    "lib_to_ndp._____(ascending=False, inplace=True)\n",
    "\n",
    "# concatenate the top and bottom values of the comparison dataframes into new ones\n",
    "l_to_c = pd._____([lib_to_con.head(), lib_to_con.tail()])\n",
    "n_to_c = pd._____([ndp_to_con.head(), ndp_to_con.tail()])\n",
    "l_to_n = pd._____([lib_to_ndp.head(), lib_to_ndp.tail()])\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_03 = l_to_c.to_numpy() # do not change this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4:\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Create a swarm plot to examine the results of the comparison between the Liberals and Conservatives. The x-axis should be the term proportions and the y-axis should be the terms themselves. Use your swarm plot to determine the word that is the most negative (Conservative) on the x-axis and submit it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_04\n",
    "\n",
    "fig, _____ = plt.subplots(figsize=(6, 4))\n",
    "sns._____(x=l_to_c, y=l_to_c.index, color='black', size=4)\n",
    "ax._____(0) # add a vertical line at 0\n",
    "plt.grid()  # add a grid to the plot to make it easier to interpret\n",
    "\n",
    "# keep in mind which party a negative value is associated with, based on which vector was the subtracted one...\n",
    "ax.set(xlabel=r'($\\longleftarrow$ Conservative Party)        (Liberal Party $\\longrightarrow$)',\n",
    "       ylabel='',\n",
    "       title='Difference of Proportions')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "most_negative_word = \"_____\"\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_04 = most_negative_word.lower() # do not change this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "In this next batch of problems, you'll expand on the concept of the previous ones by creating TF-IDF vectors for each party and comparing them using cosine similarity. Start by creating a TF-IDF dataframe, again with the speaker party column added. Print the terms with the top TF-IDF scores after sorting by each party.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Initialize the TfidfVectorizer and implement it in a very similar way to the CountVectorizer above. At the end, print the first 10 TF-IDF scores, sorted highest to lowest, for each party. This will also print the scores for those terms for the other parties. Use the printed results to determine the word that has the highest TF-IDF score for both the Conservatives and the NDP. Submit that word. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_05\n",
    "\n",
    "tfidf_vectorizer = _____(stop_words=\"english\",\n",
    "                                   lowercase=True,\n",
    "                                   max_features = 300,       # not best practice, we do this here in case of resource limitations\n",
    "                                   strip_accents='ascii')\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer._____(speeches) # Fit the model and transform the data\n",
    "\n",
    "vocabulary = tfidf_vectorizer._____() # Get the feature names\n",
    "\n",
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix)\n",
    "tfidf_df.columns = _____ # use vocabulary as column names\n",
    "\n",
    "party_scores = tfidf_df.copy()\n",
    "party_scores['speakerparty'] = df['speakerparty']\n",
    "\n",
    "party_scores = party_scores.groupby('speakerparty').agg('sum')\n",
    "party_scores = party_scores.T\n",
    "\n",
    "for party in party_scores.columns:\n",
    "    party_scores.sort_values(by = party, ascending = False, inplace = True)\n",
    "    print(party + '\\n')\n",
    "    print(party_scores.head(10))\n",
    "    print('\\n')\n",
    "\n",
    "con_and_ndp_word = \"_____\"\n",
    "    \n",
    "# Store the result in the assignment variable\n",
    "_05 = con_and_ndp_word.lower() # do not change this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Next you will calculate pair-wise cosine similarity to compare the vectors for each speaker in the data. You may have noticed that the TF-IDF scores from the last problem wound up being on different scales for each party, with the Liberals having the highest scores because they have the most speeches. This time, you will re-normalize the TF-IDF scores after adding them together, which also makes cosine similarity faster to calculate.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Create a new dataframe from the tfidf_matrix that you generated above. Be sure to add speakernames as usual, then filter the dataframe to keep only speeches by speakers with 50 or more speeches. Group the speeches by speaker, aggregating the TF-IDF vectors for each of their speeches, then use sklearn's Normalizer() to prepare the vectors for cosine similarity. Create a cosine similarity matrix by calculating the dot product of the normalized speaker score matrix and its transpose. Submit the matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_06\n",
    "\n",
    "speaker_scores = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix)   # create a new dataframe from the tfidf_matrix\n",
    "speaker_scores = speaker_scores.sparse.to_dense()                  # turn the sparse matrix into a dense one for faster aggregation runtime\n",
    "speaker_scores['speakername'] = df['speakername']                  # add the speaker names to the new dataframe\n",
    "\n",
    "# keep only speakers with 50 or more speeches to speed things up and to have vectors with a bit more term diversity\n",
    "speaker_scores = speaker_scores.groupby('speakername').filter(lambda x: len(x) >= 50)    \n",
    "speaker_scores = speaker_scores.groupby('speakername')._____('sum')        # group the speech vectors by speaker and aggregate their values\n",
    "\n",
    "normalize = _____()\n",
    "speaker_scores_n = normalize._____(speaker_scores)      # Fit the model and transform the data\n",
    "\n",
    "speaker_matrix = speaker_scores_n @ speaker_scores_n.T         # calculate the dot product of the matrix for pairwise cosine similarities\n",
    "\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_06 = speaker_matrix # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Identify the 5 most similar speakers and the 5 least similar speakers in the data.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Fill the diagonal and lower triangle of the cosine similarity matrix with np.nan values. Create a new dataframe from the matrix and make the speaker names both the index and the column names. Use df.stack() to make the dataframe 1-dimensional for a relatively simple way of finding the largest and smallest values in the whole matrix. Print the 5 highest and 5 lowest cosine comparisons. These will be the members of parliament whose speech topic composition is either most or least similar. Submit the five speaker pairs with the highest cosine similarity.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_07\n",
    "\n",
    "np._____(speaker_matrix, np.nan) # Fill the speaker matrix's diagonal with np.nan values\n",
    "speaker_matrix[np.tril_indices(speaker_matrix.shape[0], -1)] = np.nan\n",
    "speaker_df = pd.DataFrame(speaker_matrix)\n",
    "\n",
    "speaker_df.index = speaker_scores.index\n",
    "speaker_df.columns = speaker_scores.index\n",
    "\n",
    "print(speaker_df._____()._____(5)) # Stack the largest 5\n",
    "print('\\n')\n",
    "print(speaker_df._____()._____(5)) # stack the smallest 5\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_07 = speaker_df.stack().nlargest(5).to_numpy() # do not change this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Print the top-weighted terms for two speakers who were among the most similar to each other, as well as two who were the least similar to each other.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Using the normalized speaker_scores matrix, create a dataframe with speaker names as the index and feature names (terms) as the column names. Use .loc to select the row for the speaker scores you will be examining, and print the 10 most important terms along with their TF-IDF scores. Submit the word that both Anthony Rota and Bruce Stanton's share as their most important. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_08\n",
    "\n",
    "speaker_scores_df = pd.DataFrame(speaker_scores_n)\n",
    "speaker_scores_df.index = speaker_scores.index\n",
    "speaker_scores_df.columns = _____  # use vocabulary as column names\n",
    "\n",
    "top1 = speaker_scores_df.loc['Anthony Rota']._____(10)\n",
    "top2 = speaker_scores_df.loc['Bruce Stanton']._____(10)\n",
    "\n",
    "bot1 = speaker_scores_df.loc['Diane Lebouthillier']._____(10)\n",
    "bot2 = speaker_scores_df.loc['Geoff Regan']._____(10)\n",
    "\n",
    "print('Top #1 \\n')\n",
    "print(top1)\n",
    "print('\\n')\n",
    "print('Top #2 \\n')\n",
    "print(top2)\n",
    "print('\\n')\n",
    "print('Bottom #1 \\n')\n",
    "print(bot1)\n",
    "print('\\n')\n",
    "print('Bottom #2 \\n')\n",
    "print(bot2)\n",
    "print('\\n')\n",
    "\n",
    "rota_stanton_most_important_word = \"_____\"\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_08 = rota_stanton_most_important_word.lower() # do not change this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FINALIZE ASSIGNMENT\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "module = 7\n",
    "\n",
    "response_dict = {\n",
    "    \"student_id\": student_id,\n",
    "    \"grad_student\": \"EA_undergrad\",\n",
    "    \"module\": module,\n",
    "    \"responses\": [\n",
    "        _01,\n",
    "        _02,\n",
    "        _03,\n",
    "        _04,\n",
    "        _05,\n",
    "        _06,\n",
    "        _07,\n",
    "        _08,\n",
    "    ],\n",
    "    \"code_cells\": In\n",
    "}\n",
    "\n",
    "module_string = f\"module_{response_dict['module']}\"\n",
    "filename = f\"{module_string}__student_{student_id}.pkl\"\n",
    "\n",
    "with open(f\"./{filename}\", 'wb') as stream:\n",
    "    pkl.dump(response_dict, stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
